{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title Install necessary libraries\n",
        "!pip install openai ipywidgets graphviz ipython --quiet\n",
        "\n",
        "# @title Load OpenAI API Credentials\n",
        "from getpass import getpass\n",
        "import os\n",
        "import openai\n",
        "\n",
        "# Check if key is already set (e.g., in Colab secrets)\n",
        "api_key_env = os.environ.get('OPENAI_API_KEY')\n",
        "\n",
        "if api_key_env:\n",
        "    print(\"OpenAI API Key found in environment variables.\")\n",
        "    OPENAI_KEY = api_key_env\n",
        "else:\n",
        "    print(\"OpenAI API Key not found in environment variables.\")\n",
        "    # Fallback to getpass if not found\n",
        "    try:\n",
        "        OPENAI_KEY = getpass('Enter your OpenAI API Key: ')\n",
        "        os.environ['OPENAI_API_KEY'] = OPENAI_KEY\n",
        "        openai.api_key = OPENAI_KEY\n",
        "        # Test the key with a simple call (optional but recommended)\n",
        "        # client = openai.OpenAI()\n",
        "        # client.models.list()\n",
        "        print(\"OpenAI API Key configured.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to configure OpenAI API Key: {e}\")\n",
        "        OPENAI_KEY = None # Ensure it's None if setup failed\n",
        "\n",
        "# Configure the OpenAI client (using the newer openai > 1.0 library structure)\n",
        "if OPENAI_KEY:\n",
        "    try:\n",
        "      client = openai.OpenAI(api_key=OPENAI_KEY)\n",
        "      print(\"OpenAI client initialized successfully.\")\n",
        "    except Exception as e:\n",
        "      print(f\"Error initializing OpenAI client: {e}\")\n",
        "      client = None\n",
        "else:\n",
        "    print(\"Cannot initialize OpenAI client: API Key is missing.\")\n",
        "\n",
        "# @title Import other required libraries\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, Markdown\n",
        "import graphviz\n",
        "import uuid # For generating unique node IDs\n",
        "import json # For potentially storing/loading graphs\n",
        "import time # For simulating delays if needed\n",
        "import traceback # For better error reporting\n",
        "import copy # For deep copying node data if needed\n",
        "import json\n",
        "from google.colab import files # For file upload/download"
      ],
      "metadata": {
        "id": "7iYSi5Hgz44f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Node Graph Data Structure and LLM Function\n",
        "\n",
        "# Global dictionary to store our nodes and their connections\n",
        "# Structure Change: 'outputs' values are now LISTS of node IDs\n",
        "# nodes = {\n",
        "#     'node_id_1': {\n",
        "#         'name': 'User Friendly Name', 'type': 'Button', 'options': {},\n",
        "#         'outputs': {'on_click': ['node_id_2', 'node_id_4']} # <<< LIST HERE\n",
        "#     },\n",
        "#     'node_id_2': {\n",
        "#         'name': 'Ask LLM', 'type': 'LLM Program', 'options': {'prompt': '...'},\n",
        "#         'outputs': {'result': ['node_id_3']} # <<< LIST HERE\n",
        "#      },\n",
        "#      'node_id_3': {\n",
        "#         'name': 'Show Result', 'type': 'Show', 'options': {'text_template': '...'},\n",
        "#         'outputs': {} # No outputs from Show node\n",
        "#      },\n",
        "#      'node_id_4': {\n",
        "#          'name': 'Show Another', 'type': 'Show', 'options': {'text_template': 'Button also triggered this!'},\n",
        "#          'outputs': {}\n",
        "#      }\n",
        "# }\n",
        "nodes = {}\n",
        "editing_node_id = None # Track which node ID is currently being edited\n",
        "\n",
        "# --- LLM Interaction Function (No Change) ---\n",
        "def call_llm(prompt_text, model=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"Calls the OpenAI API to get a completion.\"\"\"\n",
        "    if not client:\n",
        "        return \"Error: OpenAI client not initialized. Please check API Key.\"\n",
        "    try:\n",
        "        # print(f\"\\nDEBUG: Calling LLM with prompt: '{prompt_text[:100]}...'\") # Debug print\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant executing user instructions.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt_text}\n",
        "            ]\n",
        "        )\n",
        "        result = response.choices[0].message.content.strip()\n",
        "        # print(f\"DEBUG: LLM Response received: '{result[:100]}...'\") # Debug print\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        error_message = f\"Error calling OpenAI API: {e}\"\n",
        "        print(f\"DEBUG: {error_message}\") # Debug print\n",
        "        return f\"Error: Could not get response from LLM. Details: {e}\"\n",
        "\n",
        "print(\"Node Graph Data Structure (with list outputs) and LLM function defined.\")"
      ],
      "metadata": {
        "id": "J1gQiptuz8OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Graph Visualization Function (Handles Multiple Connections)\n",
        "\n",
        "graph_output = widgets.Output()\n",
        "\n",
        "def visualize_graph():\n",
        "    \"\"\"Renders the current node graph using Graphviz, handling multiple outputs.\"\"\"\n",
        "    dot = graphviz.Digraph(comment='Node Graph', format='png')\n",
        "    dot.attr(rankdir='LR')\n",
        "\n",
        "    with dot.subgraph(name='cluster_nodes') as c:\n",
        "        c.attr(label='Nodes', style='filled', color='lightgrey')\n",
        "        for node_id, node_data in nodes.items():\n",
        "            node_label = f\"ID: {node_id}\\nType: {node_data['type']}\\nName: {node_data['name']}\"\n",
        "            # Add options preview if applicable\n",
        "            if node_data['type'] == 'LLM Program':\n",
        "                node_label += f\"\\nPrompt: {node_data['options'].get('prompt', '')[:30]}...\"\n",
        "            elif node_data['type'] == 'Show':\n",
        "                 node_label += f\"\\nText: {node_data['options'].get('text_template', '')[:30]}...\"\n",
        "\n",
        "            # Highlight node being edited\n",
        "            node_color = 'lightblue' if node_id == editing_node_id else 'white'\n",
        "            c.node(node_id, label=node_label, shape='box', style='filled', fillcolor=node_color)\n",
        "\n",
        "    # Add edges after all nodes are defined\n",
        "    for node_id, node_data in nodes.items():\n",
        "        for port_name, target_node_ids in node_data.get('outputs', {}).items():\n",
        "            # Ensure target_node_ids is a list (even if loaded from old format)\n",
        "            if isinstance(target_node_ids, str):\n",
        "                target_node_ids = [target_node_ids] # Convert old format on the fly\n",
        "\n",
        "            if isinstance(target_node_ids, list):\n",
        "                for target_node_id in target_node_ids: # Iterate through the list\n",
        "                    if target_node_id in nodes:\n",
        "                        dot.edge(node_id, target_node_id, label=port_name)\n",
        "                    else:\n",
        "                        print(f\"Warning: Connection from {node_id} ({port_name}) points to non-existent node {target_node_id}\")\n",
        "            # else: Handle potential data corruption? print warning?\n",
        "\n",
        "    with graph_output:\n",
        "        clear_output(wait=True)\n",
        "        if not nodes:\n",
        "            print(\"Graph is empty. Add some nodes!\")\n",
        "        else:\n",
        "            try:\n",
        "                display(dot)\n",
        "            except graphviz.backend.execute.ExecutableNotFound:\n",
        "                 display(Markdown(\"**Error:** `graphviz` executable not found. Rendering failed.\"))\n",
        "            except Exception as e:\n",
        "                 display(Markdown(f\"**Error rendering graph:** {e}\"))\n",
        "\n",
        "print(\"Graph Visualization function updated for multiple connections.\")\n",
        "# Initial visualization (will be empty)\n",
        "visualize_graph()"
      ],
      "metadata": {
        "id": "SgMbGQdyz9s5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Node Definition, Editing, Removal, Save/Load UI (Added Interactive Input)\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, Markdown\n",
        "import uuid\n",
        "import graphviz\n",
        "import json\n",
        "from google.colab import files\n",
        "\n",
        "# --- Global State ---\n",
        "if 'nodes' not in globals(): nodes = {}\n",
        "editing_node_id = None\n",
        "\n",
        "# --- Output Widgets ---\n",
        "edit_feedback_output = widgets.Output()\n",
        "add_node_output = widgets.Output()\n",
        "add_connection_output = widgets.Output()\n",
        "save_load_output = widgets.Output()\n",
        "if 'graph_output' not in globals(): graph_output = widgets.Output()\n",
        "if 'visualize_graph' not in globals():\n",
        "    def visualize_graph(): print(\"Error: visualize_graph not defined\")\n",
        "\n",
        "# --- VALID NODE TYPES --- Added 'Interactive Input'\n",
        "VALID_NODE_TYPES = ['Button', 'LLM Program', 'Show', 'Text Input', 'Interactive Input']\n",
        "NODE_TYPE_TO_INDEX = {t: i for i, t in enumerate(VALID_NODE_TYPES)}\n",
        "INDEX_TO_NODE_TYPE = {i: t for i, t in enumerate(VALID_NODE_TYPES)}\n",
        "\n",
        "# --- Helper: Get Node Choices ---\n",
        "def get_node_choices():\n",
        "    choices = {f\"{data['name']} ({node_id})\": node_id for node_id, data in nodes.items()}\n",
        "    return choices if choices else {\"(No nodes available)\": None}\n",
        "\n",
        "# --- Helper Functions for UI Updates ---\n",
        "def update_all_dropdowns():\n",
        "    dropdowns_to_update = ['source_node_dropdown', 'target_node_dropdown', 'select_node_edit_dropdown']\n",
        "    if not all(d in globals() for d in dropdowns_to_update): return\n",
        "    node_choices = get_node_choices()\n",
        "    for dd_name in dropdowns_to_update:\n",
        "        dropdown = globals()[dd_name]\n",
        "        current_value = dropdown.value\n",
        "        dropdown.options = node_choices\n",
        "        if current_value in node_choices.values(): dropdown.value = current_value\n",
        "        elif node_choices: dropdown.value = list(node_choices.values())[0]\n",
        "        else: dropdown.value = None\n",
        "    update_source_ports(None)\n",
        "\n",
        "def update_source_ports(change):\n",
        "    if 'source_port_dropdown' not in globals() or 'source_node_dropdown' not in globals(): return\n",
        "    selected_node_id = source_node_dropdown.value\n",
        "    port_options = []\n",
        "    if selected_node_id and selected_node_id in nodes:\n",
        "        node_type = nodes[selected_node_id]['type']\n",
        "        if node_type == 'Button': port_options = ['on_click']\n",
        "        elif node_type == 'LLM Program': port_options = ['result']\n",
        "        elif node_type == 'Text Input': port_options = ['text_out']\n",
        "        elif node_type == 'Interactive Input': port_options = ['user_input'] # Added port\n",
        "    source_port_dropdown.options = port_options\n",
        "    source_port_dropdown.value = port_options[0] if port_options else None\n",
        "\n",
        "# --- Widgets for Node Creation/Editing ---\n",
        "node_type_dropdown = widgets.Dropdown(options=VALID_NODE_TYPES, value='Button', description='Node Type:', style={'description_width': 'initial'})\n",
        "node_name_text = widgets.Text(value='My Node', placeholder='Enter node name', description='Node Name:', style={'description_width': 'initial'})\n",
        "\n",
        "# Option Boxes\n",
        "button_options_box = widgets.VBox([])\n",
        "llm_options_box = widgets.VBox([widgets.Textarea(value='Summarize.', placeholder='LLM prompt...', description='LLM Prompt:', layout={'width':'95%'}, style={'description_width':'initial'})])\n",
        "show_options_box = widgets.VBox([widgets.Textarea(value='Output: {input}', placeholder='Show text...', description='Show Text:', layout={'width':'95%'}, style={'description_width':'initial'})])\n",
        "text_input_options_box = widgets.VBox([widgets.Textarea(value='', placeholder='Static text...', description='Text Content:', layout={'width':'95%', 'height':'100px'}, style={'description_width':'initial'})])\n",
        "interactive_input_options_box = widgets.VBox([widgets.Text(value='Enter value:', placeholder='Label for input field', description='Prompt Label:', layout={'width':'95%'}, style={'description_width':'initial'})]) # Added options\n",
        "\n",
        "# Accordion including the new options box\n",
        "options_accordion = widgets.Accordion(\n",
        "    children=[button_options_box, llm_options_box, show_options_box, text_input_options_box, interactive_input_options_box], # Added interactive box\n",
        "    selected_index=0\n",
        ")\n",
        "for i, node_type in enumerate(VALID_NODE_TYPES): options_accordion.set_title(i, f'{node_type} Options')\n",
        "\n",
        "add_update_node_button = widgets.Button(description='Add Node', button_style='success', icon='plus')\n",
        "cancel_edit_button = widgets.Button(description='Cancel Edit', button_style='warning', icon='times', visible=False)\n",
        "\n",
        "# --- Widgets for Node Selection & Management ---\n",
        "select_node_edit_dropdown = widgets.Dropdown(description='Select Node:', style={'description_width': 'initial'})\n",
        "load_node_button = widgets.Button(description='Load for Edit', button_style='info', icon='edit')\n",
        "remove_node_button = widgets.Button(description='Remove Selected Node', button_style='danger', icon='trash')\n",
        "\n",
        "# --- Widgets for Adding Connections ---\n",
        "source_node_dropdown = widgets.Dropdown(description='Source Node:', style={'description_width': 'initial'})\n",
        "source_port_dropdown = widgets.Dropdown(description='Output Port:', style={'description_width': 'initial'})\n",
        "target_node_dropdown = widgets.Dropdown(description='Target Node:', style={'description_width': 'initial'})\n",
        "add_connection_button = widgets.Button(description='Add Connection', button_style='primary', icon='link')\n",
        "\n",
        "# --- Widgets for Save/Load ---\n",
        "save_filename_text = widgets.Text(value='graph.json', description='Filename:', style={'description_width': 'initial'})\n",
        "save_graph_button = widgets.Button(description='Save Graph', button_style='success', icon='save')\n",
        "load_graph_button = widgets.FileUpload(accept='.json', multiple=False, description='Load Graph', button_style='info', icon='upload')\n",
        "clear_graph_button = widgets.Button(description='Clear Graph', button_style='danger', icon='recycle')\n",
        "\n",
        "# --- Event Handlers ---\n",
        "\n",
        "def reset_node_form(clear_name=True):\n",
        "    global editing_node_id; editing_node_id = None\n",
        "    node_type_dropdown.value = 'Button'\n",
        "    if clear_name: node_name_text.value = 'My Button'\n",
        "    # Reset all option fields\n",
        "    llm_ta = next((w for w in llm_options_box.children if isinstance(w, widgets.Textarea)), None);\n",
        "    if llm_ta: llm_ta.value = 'Summarize.'\n",
        "    show_ta = next((w for w in show_options_box.children if isinstance(w, widgets.Textarea)), None);\n",
        "    if show_ta: show_ta.value = 'Output: {input}'\n",
        "    text_input_ta = next((w for w in text_input_options_box.children if isinstance(w, widgets.Textarea)), None);\n",
        "    if text_input_ta: text_input_ta.value = ''\n",
        "    interactive_input_text = next((w for w in interactive_input_options_box.children if isinstance(w, widgets.Text)), None); # Added reset\n",
        "    if interactive_input_text: interactive_input_text.value = 'Enter value:'\n",
        "\n",
        "    options_accordion.selected_index = NODE_TYPE_TO_INDEX['Button']\n",
        "    add_update_node_button.description = 'Add Node'; add_update_node_button.button_style = 'success'; add_update_node_button.icon = 'plus'\n",
        "    cancel_edit_button.visible = False; node_type_dropdown.disabled = False\n",
        "    visualize_graph()\n",
        "\n",
        "def on_cancel_edit_clicked(b):\n",
        "    with add_node_output: clear_output(wait=True); print(\"Edit cancelled.\")\n",
        "    reset_node_form()\n",
        "\n",
        "def handle_node_type_change(change):\n",
        "    if editing_node_id is None:\n",
        "        node_type = change.new; options_accordion.selected_index = NODE_TYPE_TO_INDEX.get(node_type, 0)\n",
        "        if node_type == 'LLM Program': node_name_text.value = 'Ask LLM'\n",
        "        elif node_type == 'Show': node_name_text.value = 'Display Result'\n",
        "        elif node_type == 'Button': node_name_text.value = 'Start Button'\n",
        "        elif node_type == 'Text Input': node_name_text.value = 'My Text'\n",
        "        elif node_type == 'Interactive Input': node_name_text.value = 'Get User Input' # Added suggestion\n",
        "\n",
        "\n",
        "node_type_dropdown.observe(handle_node_type_change, names='value')\n",
        "\n",
        "def on_add_update_node_clicked(b):\n",
        "    global editing_node_id\n",
        "    node_type = node_type_dropdown.value\n",
        "    node_name = node_name_text.value.strip() or f\"Untitled_{node_type}\"\n",
        "    if not node_name:\n",
        "        with add_node_output: clear_output(wait=True); print(\"❌ Node name empty.\")\n",
        "        return\n",
        "\n",
        "    options = {}\n",
        "    # Extract options based on type\n",
        "    if node_type == 'LLM Program': options['prompt'] = next((w.value for w in llm_options_box.children if isinstance(w, widgets.Textarea)), '')\n",
        "    elif node_type == 'Show': options['text_template'] = next((w.value for w in show_options_box.children if isinstance(w, widgets.Textarea)), '')\n",
        "    elif node_type == 'Text Input': options['content'] = next((w.value for w in text_input_options_box.children if isinstance(w, widgets.Textarea)), '')\n",
        "    elif node_type == 'Interactive Input': options['prompt_label'] = next((w.value for w in interactive_input_options_box.children if isinstance(w, widgets.Text)), '') # Added extraction\n",
        "\n",
        "    with add_node_output: clear_output(wait=True)\n",
        "    if editing_node_id:\n",
        "        if editing_node_id not in nodes: print(f\"❌ Error: Node {editing_node_id} not found.\"); reset_node_form(); return\n",
        "        print(f\"Updating '{nodes[editing_node_id]['name']}' ({editing_node_id})...\")\n",
        "        if nodes[editing_node_id]['type'] != node_type: print(f\"⚠️ Type change ignored.\"); node_type = nodes[editing_node_id]['type']\n",
        "        nodes[editing_node_id]['name'] = node_name; nodes[editing_node_id]['options'] = options\n",
        "        print(f\"✅ Node '{node_name}' updated.\")\n",
        "        reset_node_form(clear_name=True)\n",
        "    else:\n",
        "        new_node_id = str(uuid.uuid4())[:8]\n",
        "        while new_node_id in nodes: new_node_id = str(uuid.uuid4())[:8]\n",
        "        nodes[new_node_id] = {'name': node_name, 'type': node_type, 'options': options, 'outputs': {}}\n",
        "        print(f\"✅ Node '{node_name}' ({new_node_id}) added.\")\n",
        "        reset_node_form(clear_name=True)\n",
        "    update_all_dropdowns(); visualize_graph()\n",
        "\n",
        "def on_load_node_clicked(b):\n",
        "    global editing_node_id; selected_id = select_node_edit_dropdown.value\n",
        "    with edit_feedback_output: clear_output(wait=True)\n",
        "    if not selected_id or selected_id not in nodes: print(\"❌ Select valid node.\"); reset_node_form(); return\n",
        "    with edit_feedback_output: print(f\"Loading '{nodes[selected_id]['name']}' ({selected_id})...\")\n",
        "    editing_node_id = selected_id; node_data = nodes[selected_id]\n",
        "    node_name_text.value = node_data['name']; node_type_dropdown.value = node_data['type']\n",
        "    node_type_dropdown.disabled = True; options_accordion.selected_index = NODE_TYPE_TO_INDEX.get(node_data['type'], 0)\n",
        "    # Load options based on type\n",
        "    if node_data['type'] == 'LLM Program': next((w for w in llm_options_box.children if isinstance(w, widgets.Textarea)), None).value = node_data['options'].get('prompt', '')\n",
        "    elif node_data['type'] == 'Show': next((w for w in show_options_box.children if isinstance(w, widgets.Textarea)), None).value = node_data['options'].get('text_template', '')\n",
        "    elif node_data['type'] == 'Text Input': next((w for w in text_input_options_box.children if isinstance(w, widgets.Textarea)), None).value = node_data['options'].get('content', '')\n",
        "    elif node_data['type'] == 'Interactive Input': next((w for w in interactive_input_options_box.children if isinstance(w, widgets.Text)), None).value = node_data['options'].get('prompt_label', '') # Added loading\n",
        "\n",
        "    add_update_node_button.description = 'Update Node'; add_update_node_button.button_style = 'info'; add_update_node_button.icon = 'save'\n",
        "    cancel_edit_button.visible = True; visualize_graph()\n",
        "\n",
        "def on_remove_node_clicked(b):\n",
        "    # (Code unchanged - handles removal generically)\n",
        "    global editing_node_id; node_to_remove_id = select_node_edit_dropdown.value\n",
        "    with edit_feedback_output: clear_output(wait=True)\n",
        "    if not node_to_remove_id or node_to_remove_id not in nodes: print(\"❌ Select valid node.\"); return\n",
        "    removed_name = nodes[node_to_remove_id]['name']; print(f\"Removing '{removed_name}' ({node_to_remove_id})...\")\n",
        "    del nodes[node_to_remove_id]\n",
        "    nodes_to_check = list(nodes.keys())\n",
        "    for check_id in nodes_to_check:\n",
        "         if check_id in nodes:\n",
        "            outputs_to_check = list(nodes[check_id]['outputs'].items())\n",
        "            for port_name, target_ids in outputs_to_check:\n",
        "                if isinstance(target_ids, str): target_ids = [target_ids]\n",
        "                if isinstance(target_ids, list) and node_to_remove_id in target_ids:\n",
        "                    print(f\"  Removing conn from {check_id} [{port_name}] -> {node_to_remove_id}\")\n",
        "                    nodes[check_id]['outputs'][port_name].remove(node_to_remove_id)\n",
        "                    if not nodes[check_id]['outputs'][port_name]: del nodes[check_id]['outputs'][port_name]\n",
        "    print(f\"✅ Node '{removed_name}' removed.\")\n",
        "    if editing_node_id == node_to_remove_id: print(\"  Cancelling edit.\"); reset_node_form()\n",
        "    else: visualize_graph()\n",
        "    update_all_dropdowns()\n",
        "\n",
        "def on_add_connection_clicked(b):\n",
        "    source_id = source_node_dropdown.value; port_name = source_port_dropdown.value; target_id = target_node_dropdown.value\n",
        "    with add_connection_output: clear_output(wait=True)\n",
        "    if not all([source_id, port_name, target_id]): print(\"❌ Select source, port, target.\"); return\n",
        "    if source_id == target_id: print(\"❌ Cannot connect to self.\"); return\n",
        "    if source_id not in nodes or target_id not in nodes: print(\"❌ Node missing.\"); return\n",
        "    valid_ports = []; src_type = nodes[source_id]['type']\n",
        "    # Define valid output ports for ALL types\n",
        "    if src_type == 'Button': valid_ports = ['on_click']\n",
        "    elif src_type == 'LLM Program': valid_ports = ['result']\n",
        "    elif src_type == 'Text Input': valid_ports = ['text_out']\n",
        "    elif src_type == 'Interactive Input': valid_ports = ['user_input'] # Added port check\n",
        "\n",
        "    if port_name not in valid_ports: print(f\"❌ Invalid port '{port_name}' for type {src_type}.\"); return\n",
        "    output_dict = nodes[source_id]['outputs']\n",
        "    if port_name not in output_dict: output_dict[port_name] = []\n",
        "    if not isinstance(output_dict[port_name], list): output_dict[port_name] = [output_dict[port_name]]\n",
        "    if target_id in output_dict[port_name]: print(f\"ℹ️ Connection exists.\")\n",
        "    else: output_dict[port_name].append(target_id); print(f\"✅ Connection: {nodes[source_id]['name']} [{port_name}] -> {nodes[target_id]['name']}\")\n",
        "    visualize_graph()\n",
        "\n",
        "# --- Save/Load/Clear Handlers (Unchanged) ---\n",
        "def on_save_graph_clicked(b):\n",
        "    filename = save_filename_text.value;\n",
        "    if not filename: filename = 'graph.json'\n",
        "    if not filename.endswith('.json'): filename += '.json'\n",
        "    try:\n",
        "        json_string = json.dumps(nodes, indent=2)\n",
        "        with open(filename, 'w') as f: f.write(json_string)\n",
        "        files.download(filename)\n",
        "        with save_load_output: clear_output(wait=True); print(f\"✅ Saved & download started as '{filename}'.\")\n",
        "    except Exception as e:\n",
        "        with save_load_output: clear_output(wait=True); print(f\"❌ Error saving: {e}\")\n",
        "\n",
        "def on_load_graph_change(change):\n",
        "    global nodes\n",
        "    if not change['new']: return\n",
        "    uploaded_filename = list(change['new'].keys())[0]\n",
        "    uploaded_content = change['new'][uploaded_filename]['content']\n",
        "    with save_load_output: clear_output(wait=True); print(f\"Loading '{uploaded_filename}'...\")\n",
        "    try:\n",
        "        json_string = uploaded_content.decode('utf-8')\n",
        "        loaded_data = json.loads(json_string)\n",
        "        if not isinstance(loaded_data, dict): raise ValueError(\"Not a valid JSON dictionary.\")\n",
        "        nodes = loaded_data\n",
        "        print(f\"✅ Graph loaded from '{uploaded_filename}'.\")\n",
        "        if editing_node_id: reset_node_form()\n",
        "        update_all_dropdowns(); visualize_graph()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading graph: {e}\"); nodes = {};\n",
        "        update_all_dropdowns(); visualize_graph()\n",
        "    finally:\n",
        "        load_graph_button.value.clear()\n",
        "        load_graph_button._counter = 0\n",
        "\n",
        "def on_clear_graph_clicked(b):\n",
        "    global nodes, editing_node_id\n",
        "    with save_load_output:\n",
        "        clear_output(wait=True)\n",
        "        print(\"--> ACTION REQUIRED IN BROWSER <--\")\n",
        "        print(\"A confirmation prompt should appear.\")\n",
        "        from google.colab import output\n",
        "        confirmed = output.eval_js('confirm(\"Clear the entire graph? Cannot be undone.\")')\n",
        "        if confirmed:\n",
        "            clear_output(wait=True)\n",
        "            nodes = {}; editing_node_id = None\n",
        "            print(\"✅ Graph cleared.\")\n",
        "            reset_node_form(); update_all_dropdowns(); visualize_graph()\n",
        "        else:\n",
        "            clear_output(wait=True); print(\"Clear operation cancelled.\")\n",
        "\n",
        "# --- Attach Handlers ---\n",
        "add_update_node_button.on_click(on_add_update_node_clicked)\n",
        "cancel_edit_button.on_click(on_cancel_edit_clicked)\n",
        "load_node_button.on_click(on_load_node_clicked)\n",
        "remove_node_button.on_click(on_remove_node_clicked)\n",
        "source_node_dropdown.observe(update_source_ports, names='value')\n",
        "add_connection_button.on_click(on_add_connection_clicked)\n",
        "save_graph_button.on_click(on_save_graph_clicked)\n",
        "load_graph_button.observe(on_load_graph_change, names='value')\n",
        "clear_graph_button.on_click(on_clear_graph_clicked)\n",
        "\n",
        "# --- Layout the UI ---\n",
        "manage_nodes_section = widgets.VBox([widgets.HTML(\"<b>Manage Nodes:</b>\"), widgets.HBox([select_node_edit_dropdown, load_node_button, remove_node_button]), edit_feedback_output])\n",
        "add_update_section = widgets.VBox([widgets.HTML(\"<hr><b>Add/Update Node Details:</b>\"), node_type_dropdown, node_name_text, widgets.HTML(\"<i>Node Specific Options:</i>\"), options_accordion, widgets.HBox([add_update_node_button, cancel_edit_button]), add_node_output])\n",
        "connections_section = widgets.VBox([widgets.HTML(\"<hr><b>Add Connection:</b>\"), source_node_dropdown, source_port_dropdown, target_node_dropdown, add_connection_button, add_connection_output])\n",
        "save_load_section = widgets.VBox([widgets.HTML(\"<hr><b>Save/Load/Clear Graph:</b>\"), widgets.HBox([save_filename_text, save_graph_button]), widgets.HBox([load_graph_button, clear_graph_button]), save_load_output])\n",
        "graph_display_section = widgets.VBox([widgets.HTML(\"<hr><b>Current Graph:</b>\"), graph_output])\n",
        "\n",
        "# --- Initial Population ---\n",
        "update_all_dropdowns()\n",
        "\n",
        "# --- Display Combined UI ---\n",
        "display(widgets.VBox([manage_nodes_section, add_update_section, connections_section, save_load_section, graph_display_section]))\n",
        "\n",
        "print(\"Node Definition UI updated with 'Interactive Input' type.\")"
      ],
      "metadata": {
        "id": "ir0KRhugz_4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Play Mode Execution Logic and UI (FIX for Combining Interactive Inputs)\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, Markdown\n",
        "import uuid\n",
        "import traceback\n",
        "import time\n",
        "import collections # Needed for deque\n",
        "\n",
        "# --- Global State Access ---\n",
        "if 'nodes' not in globals(): nodes = {}\n",
        "if 'client' not in globals(): client = None\n",
        "if 'call_llm' not in globals():\n",
        "    print(\"Warning: 'call_llm' function not found globally. Defining a placeholder.\")\n",
        "    def call_llm(prompt, model=\"gpt-3.5-turbo\"):\n",
        "        print(f\"Placeholder LLM Call with prompt: {prompt[:100]}...\")\n",
        "        time.sleep(1); return \"Placeholder LLM response.\"\n",
        "\n",
        "# --- Output Widgets ---\n",
        "play_mode_button = widgets.Button(description=\"🚀 Enter Play Mode\", button_style='warning')\n",
        "play_output = widgets.Output()\n",
        "full_log_output = widgets.Output()\n",
        "\n",
        "# --- Execution State ---\n",
        "execution_queue = collections.deque()\n",
        "run_states = {} # Stores state per run_id, e.g., {'run_id_1': {'interactive_inputs': {node_id: value}, 'executed_nodes': set()}}\n",
        "active_run_id = None # Tracks the ID of the run being primarily processed (can be None)\n",
        "\n",
        "\n",
        "# --- Helper to Add to Queue ---\n",
        "def add_to_execution_queue(items):\n",
        "    \"\"\"Adds items (tuple or list of tuples) to the global queue.\"\"\"\n",
        "    global execution_queue\n",
        "    if isinstance(items, list):\n",
        "        execution_queue.extend(items)\n",
        "    else:\n",
        "        execution_queue.append(items)\n",
        "    # print(f\"DEBUG (add_to_execution_queue): Queue size now {len(execution_queue)}, Added: {items}\")\n",
        "\n",
        "# --- CORE QUEUE PROCESSING FUNCTION ---\n",
        "def process_execution_queue():\n",
        "    \"\"\"Processes items from the global execution queue for the active run.\"\"\"\n",
        "    global active_run_id, execution_queue, run_states\n",
        "    # print(f\"DEBUG (process_execution_queue START): Active run: {active_run_id}, Queue size: {len(execution_queue)}\")\n",
        "\n",
        "    max_steps_per_call = len(nodes) + 10\n",
        "    steps_processed = 0\n",
        "\n",
        "    while execution_queue and steps_processed < max_steps_per_call:\n",
        "        # Peek to check run_id\n",
        "        current_node_id_peek, _, run_id_peek, _ = execution_queue[0]\n",
        "\n",
        "        # Prioritize the active run if one is set\n",
        "        if active_run_id and run_id_peek != active_run_id:\n",
        "            # print(f\"DEBUG: Next item run {run_id_peek} != active {active_run_id}, skipping step.\")\n",
        "            # Don't process items from other runs if one is active and waiting (e.g., for interactive)\n",
        "            # This helps prevent interleaving runs in a confusing way\n",
        "             break # Stop processing until the active run can continue\n",
        "\n",
        "        # If no active run, process the next item regardless of its run_id\n",
        "        # (This handles resuming potentially multiple waiting runs)\n",
        "        current_node_id, current_input, run_id, _ = execution_queue.popleft() # We get executed_set from run_states\n",
        "\n",
        "        # Ensure state exists for this run_id\n",
        "        if run_id not in run_states:\n",
        "            print(f\"ERROR: State for run {run_id} not found! Skipping node {current_node_id}.\")\n",
        "            continue\n",
        "\n",
        "        executed_in_run = run_states[run_id]['executed_nodes'] # Get the execution set for this run\n",
        "\n",
        "        steps_processed += 1\n",
        "        # print(f\"DEBUG (DEQUEUE): Node {current_node_id}, Run {run_id}. Remaining: {len(execution_queue)}\")\n",
        "\n",
        "        # --- Standard checks ---\n",
        "        if current_node_id in executed_in_run:\n",
        "             with full_log_output: print(f\"[{run_id}] Skipping node {current_node_id} - already processed.\")\n",
        "             continue\n",
        "        executed_in_run.add(current_node_id) # Add to the run's specific set\n",
        "\n",
        "        if current_node_id not in nodes:\n",
        "            with play_output: display(Markdown(f\"⚠️ **Error:** Node `{current_node_id}` not found.\"))\n",
        "            with full_log_output: print(f\"[{run_id}] Error: Node {current_node_id} not found.\")\n",
        "            continue\n",
        "\n",
        "        node_data = nodes[current_node_id]\n",
        "        node_name = node_data['name']\n",
        "        node_type = node_data['type']\n",
        "\n",
        "        with full_log_output: print(f\"[{run_id}] Executing Node: {node_name} ({current_node_id}), Type: {node_type}\")\n",
        "\n",
        "        output_data = None\n",
        "        port_to_follow = None\n",
        "        next_nodes_info = [] # Stores tuples: (next_id, output_data, run_id) - no need for set here\n",
        "\n",
        "        try:\n",
        "            # --- Node Type Specific Logic ---\n",
        "            if node_type == 'Button':\n",
        "                output_data = current_input\n",
        "                port_to_follow = 'on_click'\n",
        "\n",
        "            elif node_type == 'Text Input':\n",
        "                node_content = node_data['options'].get('content', '')\n",
        "                # Static text nodes still output their content directly\n",
        "                output_data = {'node_name': node_name, 'text': node_content}\n",
        "                port_to_follow = 'text_out'\n",
        "                with full_log_output: print(f\"[{run_id}] Text Input node produced output.\")\n",
        "\n",
        "            elif node_type == 'Interactive Input':\n",
        "                prompt_label = node_data['options'].get('prompt_label', f'{node_name}:')\n",
        "                downstream_node_ids = []\n",
        "                connected_ids = node_data.get('outputs', {}).get('user_input', [])\n",
        "                if isinstance(connected_ids, str): connected_ids = [connected_ids]\n",
        "                if isinstance(connected_ids, list): downstream_node_ids = connected_ids\n",
        "\n",
        "                text_input_widget = widgets.Text(description=prompt_label, placeholder='Type & press Enter...', layout={'width': '80%'})\n",
        "\n",
        "                # --- Define the submit handler ---\n",
        "                # Pass the interactive node's ID to store the result correctly\n",
        "                def handle_interactive_submit(widget, interactive_node_id=current_node_id, submit_run_id=run_id):\n",
        "                    global active_run_id, run_states\n",
        "                    user_text = widget.value; widget.disabled = True\n",
        "                    with play_output: display(Markdown(f\"✅ Input for '{nodes[interactive_node_id]['name']}': `{user_text}`\"))\n",
        "                    with full_log_output: print(f\"[{submit_run_id}] User input for '{nodes[interactive_node_id]['name']}': '{user_text}'\")\n",
        "\n",
        "                    # Store the result in the run state\n",
        "                    if submit_run_id in run_states:\n",
        "                         run_states[submit_run_id]['interactive_inputs'][interactive_node_id] = user_text\n",
        "                         print(f\"DEBUG: Stored input for {interactive_node_id} in run {submit_run_id}\")\n",
        "                    else:\n",
        "                         print(f\"ERROR: Run state for {submit_run_id} not found when storing interactive input!\")\n",
        "\n",
        "                    items_to_add = []\n",
        "                    # Queue the NEXT nodes, passing None as data (LLM will fetch)\n",
        "                    for next_id in downstream_node_ids:\n",
        "                        if next_id in nodes:\n",
        "                             # Use the correct run_id captured by the handler closure\n",
        "                             new_item = (next_id, None, submit_run_id, None) # Pass None for executed_set, will get from run_states\n",
        "                             items_to_add.append(new_item)\n",
        "                        else:\n",
        "                             with full_log_output: print(f\"[{submit_run_id}] Warning: Next node {next_id} not found.\")\n",
        "\n",
        "                    if items_to_add:\n",
        "                        add_to_execution_queue(items_to_add)\n",
        "                        # IMPORTANT: Set the active run ID to THIS run before processing queue,\n",
        "                        # because processing might have stopped if multiple interactives were waiting.\n",
        "                        active_run_id = submit_run_id\n",
        "                        # print(f\"DEBUG: Set active run to {active_run_id} before processing queue.\")\n",
        "                        process_execution_queue() # Resume processing for this run\n",
        "                    # else: print(f\"DEBUG: Interactive node {interactive_node_id} had no downstream.\")\n",
        "                # --- End of handler definition ---\n",
        "\n",
        "                text_input_widget.on_submit(handle_interactive_submit) # Attach handler\n",
        "                with play_output: display(Markdown(f\"▶️ **Action Required for '{node_name}'**\")); display(text_input_widget)\n",
        "                with full_log_output: print(f\"[{run_id}] Paused execution at '{node_name}', waiting...\")\n",
        "\n",
        "                # Make THIS run the active one while waiting\n",
        "                active_run_id = run_id\n",
        "                print(f\"DEBUG: Paused. Set active run to {active_run_id}\")\n",
        "                break # Exit the 'while' loop, wait for user\n",
        "\n",
        "            elif node_type == 'LLM Program':\n",
        "                llm_node_prompt_template = node_data['options'].get('prompt', '')\n",
        "                final_prompt_parts = []; gathered_inputs_formatted = []\n",
        "                with full_log_output: print(f\"[{run_id}] LLM Node '{node_name}' gathering inputs...\")\n",
        "\n",
        "                # 1. Scan for incoming INTERACTIVE inputs from run_states\n",
        "                interactive_inputs_found = run_states.get(run_id, {}).get('interactive_inputs', {})\n",
        "                with full_log_output: print(f\"  > Checking stored interactive inputs for run {run_id}: {list(interactive_inputs_found.keys())}\")\n",
        "                for source_id, source_data in nodes.items(): # Check all nodes\n",
        "                     # Find nodes connected TO this LLM node\n",
        "                    source_outputs = source_data.get('outputs', {})\n",
        "                    for port_name, target_ids in source_outputs.items():\n",
        "                        if isinstance(target_ids, str): target_ids = [target_ids]\n",
        "                        if current_node_id in target_ids: # Is current LLM node a target?\n",
        "                            # Check if the source was an Interactive Input AND its result is stored\n",
        "                            if source_data['type'] == 'Interactive Input' and source_id in interactive_inputs_found:\n",
        "                                stored_text = interactive_inputs_found[source_id]\n",
        "                                input_node_name = source_data['name']\n",
        "                                gathered_inputs_formatted.append(f\"Input from interactive node '{input_node_name}':\\n{stored_text}\")\n",
        "                                with full_log_output: print(f\"  > Found stored interactive input from '{input_node_name}' ({source_id})\")\n",
        "\n",
        "                # 2. Scan for connected STATIC Text Input nodes\n",
        "                # with full_log_output: print(f\"[{run_id}] LLM Node '{node_name}' scanning for connected STATIC Text Inputs...\")\n",
        "                for source_id, source_data in nodes.items():\n",
        "                    if source_data['type'] == 'Text Input':\n",
        "                        source_outputs = source_data.get('outputs', {})\n",
        "                        text_out_targets = source_outputs.get('text_out', [])\n",
        "                        if isinstance(text_out_targets, str): text_out_targets = [text_out_targets]\n",
        "                        if current_node_id in text_out_targets:\n",
        "                             text_content = source_data['options'].get('content', '')\n",
        "                             text_node_name = source_data['name']\n",
        "                             gathered_inputs_formatted.append(f\"Input from static node '{text_node_name}':\\n{text_content}\")\n",
        "                             with full_log_output: print(f\"  > Found connected static text from '{text_node_name}' ({source_id})\")\n",
        "\n",
        "\n",
        "                # 3. Add direct input if it's not from Interactive/Static path handled above\n",
        "                # (This logic might be redundant now, depends on desired behavior)\n",
        "                # if isinstance(current_input, str) and current_input:\n",
        "                #      is_handled = False # Check if this input was already gathered\n",
        "                #      # ... logic to check if current_input matches a gathered one ...\n",
        "                #      if not is_handled:\n",
        "                #          gathered_inputs_formatted.append(f\"Input from Previous Node:\\n{current_input}\")\n",
        "                #          with full_log_output: print(f\"  > Including direct string input.\")\n",
        "\n",
        "                # 4. Construct the context block\n",
        "                if gathered_inputs_formatted:\n",
        "                    context_block = \"\\n\\n---\\n\".join(gathered_inputs_formatted)\n",
        "                    final_prompt_parts.append(\"--- Input Context ---\"); final_prompt_parts.append(context_block); final_prompt_parts.append(\"---------------------\\n\")\n",
        "                else:\n",
        "                     with full_log_output: print(f\"[{run_id}] LLM node received no specific input context.\")\n",
        "\n",
        "                # 5. Add the LLM node's own instruction\n",
        "                instruction = llm_node_prompt_template.replace('{input}', '').strip()\n",
        "                if instruction: final_prompt_parts.append(f\"User instruction: {instruction}\")\n",
        "                else: final_prompt_parts.append(\"User instruction: (None)\")\n",
        "                final_prompt = \"\\n\".join(final_prompt_parts)\n",
        "\n",
        "                # 6. Call LLM\n",
        "                if client:\n",
        "                    with full_log_output: print(f\"[{run_id}] Calling LLM with combined prompt:\\n{final_prompt[:600]}...\")\n",
        "                    output_data = call_llm(final_prompt)\n",
        "                else:\n",
        "                    output_data = \"Error: OpenAI client not initialized.\"\n",
        "                    with play_output: display(Markdown(f\"⚠️ LLM Error: Client missing.\"))\n",
        "                port_to_follow = 'result'\n",
        "\n",
        "            elif node_type == 'Show':\n",
        "                text_template = node_data['options'].get('text_template', 'Output: {input}')\n",
        "                display_input_str = str(current_input) if current_input is not None else \"\"\n",
        "                display_text = text_template.replace('{input}', display_input_str)\n",
        "                with play_output: display(Markdown(f\"**{node_name}:**\\n\\n```\\n{display_text}\\n```\"))\n",
        "                output_data = display_text\n",
        "                port_to_follow = None\n",
        "\n",
        "            else: # Unknown node type\n",
        "                with play_output: display(Markdown(f\"❓ Unknown Type: `{node_type}`\"))\n",
        "                with full_log_output: print(f\"[{run_id}] Error: Unknown type {node_type}\")\n",
        "                port_to_follow = None\n",
        "\n",
        "            # --- Find Next Nodes to Queue ---\n",
        "            if port_to_follow and port_to_follow in node_data.get('outputs', {}):\n",
        "                potential_next_ids = node_data['outputs'][port_to_follow]\n",
        "                if isinstance(potential_next_ids, str): potential_next_ids = [potential_next_ids]\n",
        "                if isinstance(potential_next_ids, list):\n",
        "                    for next_id in potential_next_ids:\n",
        "                         if next_id and next_id in nodes:\n",
        "                            # Pass output_data from current node, and run_id. Set executed_set to None.\n",
        "                            next_nodes_info.append((next_id, output_data, run_id, None))\n",
        "                         elif next_id:\n",
        "                             with full_log_output: print(f\"[{run_id}] Warning: Target node {next_id} not found.\")\n",
        "\n",
        "        except Exception as e: # Catch errors during node execution\n",
        "             tb_str = traceback.format_exc()\n",
        "             error_md = Markdown(f\"💥 Exec Error `{node_name}` ({current_node_id}):\\n```\\n{e}\\n{tb_str[:500]}...\\n```\")\n",
        "             with play_output: display(error_md)\n",
        "             with full_log_output: print(f\"[{run_id}] FATAL ERROR node {current_node_id}: {e}\\n{tb_str}\")\n",
        "             continue\n",
        "\n",
        "        # --- Queue Next Nodes (if any found) ---\n",
        "        if next_nodes_info:\n",
        "            add_to_execution_queue(next_nodes_info)\n",
        "\n",
        "    # --- End of while execution_queue loop for this call ---\n",
        "    # Check if the queue is empty AND we had an active run ID before clearing it\n",
        "    if not execution_queue and active_run_id and active_run_id in run_states:\n",
        "         # print(f\"DEBUG: Run {active_run_id} finished and queue empty. Removing state.\")\n",
        "         del run_states[active_run_id] # Clean up state for completed run\n",
        "         active_run_id = None # Clear active run ID only when queue is truly empty for it\n",
        "    # elif not execution_queue:\n",
        "    #     active_run_id = None # Clear if queue is empty even if no run was active\n",
        "    # else: print(f\"DEBUG: process_execution_queue loop exited. Queue size: {len(execution_queue)}\")\n",
        "\n",
        "\n",
        "# --- Function to Start a Flow ---\n",
        "def execute_node_flow_start(start_node_id, input_data=None):\n",
        "    \"\"\"Sets up and starts processing a new flow execution.\"\"\"\n",
        "    global execution_queue, active_run_id, run_states\n",
        "    new_run_id = str(uuid.uuid4())[:6]\n",
        "\n",
        "    # Clean up old finished states (simple approach)\n",
        "    # A more robust approach might use timestamps or explicit cleanup nodes\n",
        "    runs_to_clear = [r_id for r_id in run_states if not any(item[2] == r_id for item in execution_queue)]\n",
        "    for r_id in runs_to_clear:\n",
        "        if r_id != active_run_id: # Don't clear the currently active run state yet\n",
        "             print(f\"DEBUG: Cleaning up state for old run {r_id}\")\n",
        "             del run_states[r_id]\n",
        "\n",
        "\n",
        "    if active_run_id and execution_queue:\n",
        "        print(f\"Warning: Starting new run {new_run_id} while run {active_run_id} might be waiting.\")\n",
        "\n",
        "    active_run_id = new_run_id # Set the new active run\n",
        "    # Initialize state for the new run\n",
        "    run_states[active_run_id] = {\n",
        "        'interactive_inputs': {},\n",
        "        'executed_nodes': set()\n",
        "    }\n",
        "\n",
        "\n",
        "    with full_log_output: print(f\"\\n--- Triggering New Execution Run {active_run_id} from Node {start_node_id} ---\")\n",
        "\n",
        "    # Add starting item - pass None for executed_set initially, will be retrieved from run_states\n",
        "    add_to_execution_queue([(start_node_id, input_data, active_run_id, None)])\n",
        "    # print(f\"DEBUG (execute_node_flow_start): Added start node {start_node_id} to queue for run {active_run_id}.\")\n",
        "\n",
        "    process_execution_queue() # Start processing\n",
        "\n",
        "\n",
        "# --- Play Mode Button Handler ---\n",
        "def on_play_mode_clicked(b):\n",
        "    with play_output: clear_output(wait=True); display(Markdown(\"--- **Play Mode Activated** ---\")); print(\"Scanning for Button nodes...\")\n",
        "    with full_log_output: clear_output(wait=True); print(\"--- Play Mode Log ---\")\n",
        "    buttons_to_display = []\n",
        "    found_buttons = False\n",
        "    for node_id, node_data in nodes.items():\n",
        "        if node_data.get('type') == 'Button':\n",
        "            found_buttons = True\n",
        "            try:\n",
        "                button_widget = widgets.Button(description=f\"▶️ {node_data.get('name', 'Button')}\", button_style='primary', tooltip=f\"Trigger {node_id}\")\n",
        "                def handle_play_button_click(b_instance, n_id=node_id):\n",
        "                    with play_output: display(Markdown(f\"▶️ *Clicked '{nodes[n_id]['name']}'...*\"))\n",
        "                    try: execute_node_flow_start(n_id, input_data=None)\n",
        "                    except Exception as e_flow_start:\n",
        "                         tb_flow_start = traceback.format_exc()\n",
        "                         err_start_md = Markdown(f\"💥 Flow Start Error `{nodes[n_id]['name']}` ({n_id}):\\n```\\n{e_flow_start}\\n{tb_flow_start[:500]}...\\n```\")\n",
        "                         with play_output: display(err_start_md)\n",
        "                         with full_log_output: print(f\"FATAL ERROR starting flow from {n_id}: {e_flow_start}\\n{tb_flow_start}\")\n",
        "                button_widget.on_click(handle_play_button_click)\n",
        "                buttons_to_display.append(button_widget)\n",
        "            except Exception as e_create:\n",
        "                 print(f\"ERROR creating button for {node_id}: {e_create}\")\n",
        "                 with play_output: display(Markdown(f\"⚠️ Error creating button for `{node_data.get('name', node_id)}`\"))\n",
        "    with play_output:\n",
        "        if not found_buttons: display(Markdown(\"No 'Button' nodes found.\"))\n",
        "        else:\n",
        "            # print(f\"Displaying {len(buttons_to_display)} buttons...\")\n",
        "            for btn in buttons_to_display: display(btn)\n",
        "\n",
        "play_mode_button.on_click(on_play_mode_clicked)\n",
        "\n",
        "# Display UI Elements\n",
        "display(widgets.VBox([\n",
        "    play_mode_button,\n",
        "    widgets.HTML(\"<hr><b>Play Area:</b>\"), play_output,\n",
        "    widgets.HTML(\"<hr><b>Execution Log:</b>\"), full_log_output\n",
        "]))\n",
        "\n",
        "print(\"Play Mode UI and Logic updated (FIX for Combining Interactive Inputs).\")"
      ],
      "metadata": {
        "id": "RLIioW98XLmm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}